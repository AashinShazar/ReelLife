{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 450 training part images.\n",
      "There are 29 validation part images.\n",
      "There are 100 test part images.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_files       \n",
    "from keras.utils import np_utils\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "\n",
    "bigPath = '/home/aashinshazar/beta/cookiesinthejar/'\n",
    "\n",
    "# define function to load train, test, and validation datasets\n",
    "def load_dataset(path):\n",
    "    data = load_files(path)\n",
    "    part_files = np.array(data['filenames'])\n",
    "    part_targets = np_utils.to_categorical(np.array(data['target']), 2)\n",
    "    return part_files, part_targets\n",
    "\n",
    "# load train, test, and validation datasets\n",
    "train_files, train_targets = load_dataset(bigPath + 'train')\n",
    "test_files, test_targets = load_dataset(bigPath +'test')\n",
    "valid_files, valid_targets = load_dataset(bigPath +'valid')\n",
    "\n",
    "print('There are %d training part images.' % len(train_files))\n",
    "print('There are %d validation part images.' % len(valid_files))\n",
    "print('There are %d test part images.'% len(test_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 450/450 [00:02<00:00, 173.90it/s]\n",
      "100%|██████████| 29/29 [00:00<00:00, 181.72it/s]\n",
      "100%|██████████| 100/100 [00:00<00:00, 178.67it/s]\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing import image                  \n",
    "from tqdm import tqdm\n",
    "\n",
    "def path_to_tensor(img_path):\n",
    "    # loads RGB image as PIL.Image.Image type\n",
    "    img = image.load_img(img_path, target_size=(224, 224))\n",
    "    # convert PIL.Image.Image type to 3D tensor with shape (224, 224, 3)\n",
    "    x = image.img_to_array(img)\n",
    "    # convert 3D tensor to 4D tensor with shape (1, 224, 224, 3) and return 4D tensor\n",
    "    return np.expand_dims(x, axis=0)\n",
    "\n",
    "def paths_to_tensor(img_paths):\n",
    "    list_of_tensors = [path_to_tensor(img_path) for img_path in tqdm(img_paths)]\n",
    "    return np.vstack(list_of_tensors)\n",
    "\n",
    "from PIL import ImageFile                            \n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True                 \n",
    "\n",
    "# pre-process the data for Keras\n",
    "train_tensors = paths_to_tensor(train_files).astype('float32')/255\n",
    "valid_tensors = paths_to_tensor(valid_files).astype('float32')/255\n",
    "test_tensors = paths_to_tensor(test_files).astype('float32')/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.7/inception_resnet_v2_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "219062272/219055592 [==============================] - 27s 0us/step\n"
     ]
    }
   ],
   "source": [
    "from keras import applications\n",
    "\n",
    "model = applications.InceptionResNetV2(include_top=False, weights='imagenet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_6 (Flatten)          (None, 150528)            0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 256)               38535424  \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 2)                 514       \n",
      "=================================================================\n",
      "Total params: 38,535,938\n",
      "Trainable params: 38,535,938\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Conv2D, MaxPooling2D, GlobalAveragePooling2D\n",
    "from keras.layers import Dropout, Flatten, Dense\n",
    "from keras.models import Sequential\n",
    "import tensorflow as tf\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Flatten(input_shape=(224, 224, 3)))\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(2, activation='sigmoid'))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 450 samples, validate on 29 samples\n",
      "Epoch 1/100\n",
      " - 10s - loss: 6.8613 - acc: 0.5489 - val_loss: 9.3970 - val_acc: 0.4138\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 9.39704, saving model to InceptionResNetV2_Bench.hdf5\n",
      "Epoch 2/100\n",
      " - 0s - loss: 7.1042 - acc: 0.5556 - val_loss: 9.3970 - val_acc: 0.4138\n",
      "\n",
      "Epoch 00002: val_loss did not improve from 9.39704\n",
      "Epoch 3/100\n",
      " - 0s - loss: 7.0889 - acc: 0.5578 - val_loss: 9.3970 - val_acc: 0.4138\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 9.39704\n",
      "Epoch 4/100\n",
      " - 0s - loss: 7.0889 - acc: 0.5578 - val_loss: 9.3970 - val_acc: 0.4138\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 9.39704\n",
      "Epoch 5/100\n",
      " - 0s - loss: 7.0889 - acc: 0.5578 - val_loss: 9.3970 - val_acc: 0.4138\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 9.39704\n",
      "Epoch 6/100\n",
      " - 0s - loss: 7.0889 - acc: 0.5578 - val_loss: 9.3970 - val_acc: 0.4138\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 9.39704\n",
      "Epoch 7/100\n",
      " - 0s - loss: 7.0889 - acc: 0.5578 - val_loss: 9.3970 - val_acc: 0.4138\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 9.39704\n",
      "Epoch 8/100\n",
      " - 0s - loss: 7.0889 - acc: 0.5578 - val_loss: 9.3970 - val_acc: 0.4138\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 9.39704\n",
      "Epoch 9/100\n",
      " - 0s - loss: 7.0889 - acc: 0.5578 - val_loss: 9.3970 - val_acc: 0.4138\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 9.39704\n",
      "Epoch 10/100\n",
      " - 0s - loss: 7.0889 - acc: 0.5578 - val_loss: 9.3970 - val_acc: 0.4138\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 9.39704\n",
      "Epoch 11/100\n",
      " - 0s - loss: 7.0889 - acc: 0.5578 - val_loss: 9.3970 - val_acc: 0.4138\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 9.39704\n",
      "Epoch 12/100\n",
      " - 0s - loss: 7.0889 - acc: 0.5578 - val_loss: 9.3970 - val_acc: 0.4138\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 9.39704\n",
      "Epoch 13/100\n",
      " - 0s - loss: 7.0889 - acc: 0.5578 - val_loss: 9.3970 - val_acc: 0.4138\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 9.39704\n",
      "Epoch 14/100\n",
      " - 0s - loss: 7.0889 - acc: 0.5578 - val_loss: 9.3970 - val_acc: 0.4138\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 9.39704\n",
      "Epoch 15/100\n",
      " - 0s - loss: 7.0889 - acc: 0.5578 - val_loss: 9.3970 - val_acc: 0.4138\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 9.39704\n",
      "Epoch 16/100\n",
      " - 0s - loss: 7.0889 - acc: 0.5578 - val_loss: 9.3970 - val_acc: 0.4138\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 9.39704\n",
      "Epoch 17/100\n",
      " - 0s - loss: 7.0889 - acc: 0.5578 - val_loss: 9.3970 - val_acc: 0.4138\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 9.39704\n",
      "Epoch 18/100\n",
      " - 0s - loss: 7.0889 - acc: 0.5578 - val_loss: 9.3970 - val_acc: 0.4138\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 9.39704\n",
      "Epoch 19/100\n",
      " - 0s - loss: 7.0889 - acc: 0.5578 - val_loss: 9.3970 - val_acc: 0.4138\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 9.39704\n",
      "Epoch 20/100\n",
      " - 0s - loss: 7.0889 - acc: 0.5578 - val_loss: 9.3970 - val_acc: 0.4138\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 9.39704\n",
      "Epoch 21/100\n",
      " - 0s - loss: 7.0889 - acc: 0.5578 - val_loss: 9.3970 - val_acc: 0.4138\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 9.39704\n",
      "Epoch 22/100\n",
      " - 0s - loss: 7.0889 - acc: 0.5578 - val_loss: 9.3970 - val_acc: 0.4138\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 9.39704\n",
      "Epoch 23/100\n",
      " - 0s - loss: 7.0889 - acc: 0.5578 - val_loss: 9.3970 - val_acc: 0.4138\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 9.39704\n",
      "Epoch 24/100\n",
      " - 0s - loss: 7.0889 - acc: 0.5578 - val_loss: 9.3970 - val_acc: 0.4138\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 9.39704\n",
      "Epoch 25/100\n",
      " - 0s - loss: 7.0889 - acc: 0.5578 - val_loss: 9.3970 - val_acc: 0.4138\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 9.39704\n",
      "Epoch 26/100\n",
      " - 0s - loss: 7.0889 - acc: 0.5578 - val_loss: 9.3970 - val_acc: 0.4138\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 9.39704\n",
      "Epoch 27/100\n",
      " - 0s - loss: 7.0889 - acc: 0.5578 - val_loss: 9.3970 - val_acc: 0.4138\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 9.39704\n",
      "Epoch 28/100\n",
      " - 0s - loss: 7.0889 - acc: 0.5578 - val_loss: 9.3970 - val_acc: 0.4138\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 9.39704\n",
      "Epoch 29/100\n",
      " - 0s - loss: 7.0889 - acc: 0.5578 - val_loss: 9.3970 - val_acc: 0.4138\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 9.39704\n",
      "Epoch 30/100\n",
      " - 0s - loss: 7.0889 - acc: 0.5578 - val_loss: 9.3970 - val_acc: 0.4138\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 9.39704\n",
      "Epoch 31/100\n",
      " - 0s - loss: 7.0889 - acc: 0.5578 - val_loss: 9.3970 - val_acc: 0.4138\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 9.39704\n",
      "Epoch 32/100\n",
      " - 0s - loss: 7.0889 - acc: 0.5578 - val_loss: 9.3970 - val_acc: 0.4138\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 9.39704\n",
      "Epoch 33/100\n",
      " - 0s - loss: 7.0889 - acc: 0.5578 - val_loss: 9.3970 - val_acc: 0.4138\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 9.39704\n",
      "Epoch 34/100\n",
      " - 0s - loss: 7.0889 - acc: 0.5578 - val_loss: 9.3970 - val_acc: 0.4138\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 9.39704\n",
      "Epoch 35/100\n",
      " - 0s - loss: 7.0889 - acc: 0.5578 - val_loss: 9.3970 - val_acc: 0.4138\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 9.39704\n",
      "Epoch 36/100\n",
      " - 0s - loss: 7.0889 - acc: 0.5578 - val_loss: 9.3970 - val_acc: 0.4138\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 9.39704\n",
      "Epoch 37/100\n",
      " - 0s - loss: 7.0889 - acc: 0.5578 - val_loss: 9.3970 - val_acc: 0.4138\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 9.39704\n",
      "Epoch 38/100\n",
      " - 0s - loss: 7.0889 - acc: 0.5578 - val_loss: 9.3970 - val_acc: 0.4138\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 9.39704\n",
      "Epoch 39/100\n",
      " - 0s - loss: 7.0889 - acc: 0.5578 - val_loss: 9.3970 - val_acc: 0.4138\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 9.39704\n",
      "Epoch 40/100\n",
      " - 0s - loss: 7.0889 - acc: 0.5578 - val_loss: 9.3970 - val_acc: 0.4138\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 9.39704\n",
      "Epoch 41/100\n",
      " - 0s - loss: 7.0889 - acc: 0.5578 - val_loss: 9.3970 - val_acc: 0.4138\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 9.39704\n",
      "Epoch 42/100\n",
      " - 0s - loss: 7.0889 - acc: 0.5578 - val_loss: 9.3970 - val_acc: 0.4138\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 9.39704\n",
      "Epoch 43/100\n",
      " - 0s - loss: 7.0889 - acc: 0.5578 - val_loss: 9.3970 - val_acc: 0.4138\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 9.39704\n",
      "Epoch 44/100\n",
      " - 0s - loss: 7.0889 - acc: 0.5578 - val_loss: 9.3970 - val_acc: 0.4138\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 9.39704\n",
      "Epoch 45/100\n",
      " - 0s - loss: 7.0889 - acc: 0.5578 - val_loss: 9.3970 - val_acc: 0.4138\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 9.39704\n",
      "Epoch 46/100\n",
      " - 0s - loss: 7.0889 - acc: 0.5578 - val_loss: 9.3970 - val_acc: 0.4138\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 9.39704\n",
      "Epoch 47/100\n",
      " - 0s - loss: 7.0889 - acc: 0.5578 - val_loss: 9.3970 - val_acc: 0.4138\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 9.39704\n",
      "Epoch 48/100\n",
      " - 0s - loss: 7.0889 - acc: 0.5578 - val_loss: 9.3970 - val_acc: 0.4138\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 9.39704\n",
      "Epoch 49/100\n",
      " - 0s - loss: 7.0889 - acc: 0.5578 - val_loss: 9.3970 - val_acc: 0.4138\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 9.39704\n",
      "Epoch 50/100\n",
      " - 0s - loss: 7.0889 - acc: 0.5578 - val_loss: 9.3970 - val_acc: 0.4138\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 9.39704\n",
      "Epoch 51/100\n",
      " - 0s - loss: 7.0889 - acc: 0.5578 - val_loss: 9.3970 - val_acc: 0.4138\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 9.39704\n",
      "Epoch 52/100\n",
      " - 0s - loss: 7.0889 - acc: 0.5578 - val_loss: 9.3970 - val_acc: 0.4138\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 9.39704\n",
      "Epoch 53/100\n",
      " - 0s - loss: 7.0889 - acc: 0.5578 - val_loss: 9.3970 - val_acc: 0.4138\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 9.39704\n",
      "Epoch 54/100\n",
      " - 0s - loss: 7.0889 - acc: 0.5578 - val_loss: 9.3970 - val_acc: 0.4138\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 9.39704\n",
      "Epoch 55/100\n",
      " - 0s - loss: 7.0889 - acc: 0.5578 - val_loss: 9.3970 - val_acc: 0.4138\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 9.39704\n",
      "Epoch 56/100\n",
      " - 0s - loss: 7.0889 - acc: 0.5578 - val_loss: 9.3970 - val_acc: 0.4138\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 9.39704\n",
      "Epoch 57/100\n",
      " - 0s - loss: 7.0889 - acc: 0.5578 - val_loss: 9.3970 - val_acc: 0.4138\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 9.39704\n",
      "Epoch 58/100\n",
      " - 0s - loss: 7.0889 - acc: 0.5578 - val_loss: 9.3970 - val_acc: 0.4138\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 9.39704\n",
      "Epoch 59/100\n",
      " - 1s - loss: 7.0889 - acc: 0.5578 - val_loss: 9.3970 - val_acc: 0.4138\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 9.39704\n",
      "Epoch 60/100\n",
      " - 0s - loss: 7.0889 - acc: 0.5578 - val_loss: 9.3970 - val_acc: 0.4138\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00060: val_loss did not improve from 9.39704\n",
      "Epoch 61/100\n",
      " - 1s - loss: 7.0889 - acc: 0.5578 - val_loss: 9.3970 - val_acc: 0.4138\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 9.39704\n",
      "Epoch 62/100\n",
      " - 0s - loss: 7.0889 - acc: 0.5578 - val_loss: 9.3970 - val_acc: 0.4138\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 9.39704\n",
      "Epoch 63/100\n",
      " - 1s - loss: 7.0889 - acc: 0.5578 - val_loss: 9.3970 - val_acc: 0.4138\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 9.39704\n",
      "Epoch 64/100\n",
      " - 1s - loss: 7.0889 - acc: 0.5578 - val_loss: 9.3970 - val_acc: 0.4138\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 9.39704\n",
      "Epoch 65/100\n",
      " - 0s - loss: 7.0889 - acc: 0.5578 - val_loss: 9.3970 - val_acc: 0.4138\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 9.39704\n",
      "Epoch 66/100\n",
      " - 0s - loss: 7.0889 - acc: 0.5578 - val_loss: 9.3970 - val_acc: 0.4138\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 9.39704\n",
      "Epoch 67/100\n",
      " - 0s - loss: 7.0889 - acc: 0.5578 - val_loss: 9.3970 - val_acc: 0.4138\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 9.39704\n",
      "Epoch 68/100\n",
      " - 0s - loss: 7.0889 - acc: 0.5578 - val_loss: 9.3970 - val_acc: 0.4138\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 9.39704\n",
      "Epoch 69/100\n",
      " - 0s - loss: 7.0889 - acc: 0.5578 - val_loss: 9.3970 - val_acc: 0.4138\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 9.39704\n",
      "Epoch 70/100\n",
      " - 1s - loss: 7.0889 - acc: 0.5578 - val_loss: 9.3970 - val_acc: 0.4138\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 9.39704\n",
      "Epoch 71/100\n",
      " - 1s - loss: 7.0889 - acc: 0.5578 - val_loss: 9.3970 - val_acc: 0.4138\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 9.39704\n",
      "Epoch 72/100\n",
      " - 0s - loss: 7.0889 - acc: 0.5578 - val_loss: 9.3970 - val_acc: 0.4138\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 9.39704\n",
      "Epoch 73/100\n",
      " - 0s - loss: 7.0889 - acc: 0.5578 - val_loss: 9.3970 - val_acc: 0.4138\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 9.39704\n",
      "Epoch 74/100\n",
      " - 0s - loss: 7.0889 - acc: 0.5578 - val_loss: 9.3970 - val_acc: 0.4138\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 9.39704\n",
      "Epoch 75/100\n",
      " - 0s - loss: 7.0889 - acc: 0.5578 - val_loss: 9.3970 - val_acc: 0.4138\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 9.39704\n",
      "Epoch 76/100\n",
      " - 0s - loss: 7.0889 - acc: 0.5578 - val_loss: 9.3970 - val_acc: 0.4138\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 9.39704\n",
      "Epoch 77/100\n",
      " - 0s - loss: 7.0889 - acc: 0.5578 - val_loss: 9.3970 - val_acc: 0.4138\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 9.39704\n",
      "Epoch 78/100\n",
      " - 1s - loss: 7.0889 - acc: 0.5578 - val_loss: 9.3970 - val_acc: 0.4138\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 9.39704\n",
      "Epoch 79/100\n",
      " - 0s - loss: 7.0889 - acc: 0.5578 - val_loss: 9.3970 - val_acc: 0.4138\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 9.39704\n",
      "Epoch 80/100\n",
      " - 0s - loss: 7.0889 - acc: 0.5578 - val_loss: 9.3970 - val_acc: 0.4138\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 9.39704\n",
      "Epoch 81/100\n",
      " - 0s - loss: 7.0889 - acc: 0.5578 - val_loss: 9.3970 - val_acc: 0.4138\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 9.39704\n",
      "Epoch 82/100\n",
      " - 0s - loss: 7.0889 - acc: 0.5578 - val_loss: 9.3970 - val_acc: 0.4138\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 9.39704\n",
      "Epoch 83/100\n",
      " - 0s - loss: 7.0889 - acc: 0.5578 - val_loss: 9.3970 - val_acc: 0.4138\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 9.39704\n",
      "Epoch 84/100\n",
      " - 0s - loss: 7.0889 - acc: 0.5578 - val_loss: 9.3970 - val_acc: 0.4138\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 9.39704\n",
      "Epoch 85/100\n",
      " - 0s - loss: 7.0889 - acc: 0.5578 - val_loss: 9.3970 - val_acc: 0.4138\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 9.39704\n",
      "Epoch 86/100\n",
      " - 0s - loss: 7.0889 - acc: 0.5578 - val_loss: 9.3970 - val_acc: 0.4138\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 9.39704\n",
      "Epoch 87/100\n",
      " - 0s - loss: 7.0889 - acc: 0.5578 - val_loss: 9.3970 - val_acc: 0.4138\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 9.39704\n",
      "Epoch 88/100\n",
      " - 0s - loss: 7.0889 - acc: 0.5578 - val_loss: 9.3970 - val_acc: 0.4138\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 9.39704\n",
      "Epoch 89/100\n",
      " - 0s - loss: 7.0889 - acc: 0.5578 - val_loss: 9.3970 - val_acc: 0.4138\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 9.39704\n",
      "Epoch 90/100\n",
      " - 0s - loss: 7.0889 - acc: 0.5578 - val_loss: 9.3970 - val_acc: 0.4138\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 9.39704\n",
      "Epoch 91/100\n",
      " - 0s - loss: 7.0889 - acc: 0.5578 - val_loss: 9.3970 - val_acc: 0.4138\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 9.39704\n",
      "Epoch 92/100\n",
      " - 0s - loss: 7.0889 - acc: 0.5578 - val_loss: 9.3970 - val_acc: 0.4138\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 9.39704\n",
      "Epoch 93/100\n",
      " - 0s - loss: 7.0889 - acc: 0.5578 - val_loss: 9.3970 - val_acc: 0.4138\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 9.39704\n",
      "Epoch 94/100\n",
      " - 0s - loss: 7.0889 - acc: 0.5578 - val_loss: 9.3970 - val_acc: 0.4138\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 9.39704\n",
      "Epoch 95/100\n",
      " - 0s - loss: 7.0889 - acc: 0.5578 - val_loss: 9.3970 - val_acc: 0.4138\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 9.39704\n",
      "Epoch 96/100\n",
      " - 0s - loss: 7.0889 - acc: 0.5578 - val_loss: 9.3970 - val_acc: 0.4138\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 9.39704\n",
      "Epoch 97/100\n",
      " - 0s - loss: 7.0889 - acc: 0.5578 - val_loss: 9.3970 - val_acc: 0.4138\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 9.39704\n",
      "Epoch 98/100\n",
      " - 0s - loss: 7.0889 - acc: 0.5578 - val_loss: 9.3970 - val_acc: 0.4138\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 9.39704\n",
      "Epoch 99/100\n",
      " - 0s - loss: 7.0889 - acc: 0.5578 - val_loss: 9.3970 - val_acc: 0.4138\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 9.39704\n",
      "Epoch 100/100\n",
      " - 0s - loss: 7.0889 - acc: 0.5578 - val_loss: 9.3970 - val_acc: 0.4138\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 9.39704\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy\n",
    "\n",
    "epochs = 100\n",
    "batch_size=20\n",
    "\n",
    "bestModelSavedName = \"InceptionResNetV2_Bench.hdf5\"\n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath=bestModelSavedName, \n",
    "                               verbose=1, save_best_only=True)\n",
    "\n",
    "history = model.fit(train_tensors, train_targets, \n",
    "          validation_data=(valid_tensors, valid_targets), \n",
    "          epochs=epochs, batch_size=batch_size, callbacks=[checkpointer], verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAEWCAYAAACXGLsWAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAAH8RJREFUeJzt3X2cHnV97vHPRR5IAoGEJDwl0cQakcfysEas2uIDPaFIwFIIBVTsoWkrHMAXtI1tT0W0p9pjaatyVEQsVkrACBhFSQMlWstDs4EUSAImUmg2PC0JhEQTSOA6f8xsvFk2OzdhJ/dm93q/XvvKPb/5zcx3MrBX5jf3zMg2ERERvdmt1QVERET/l7CIiIhKCYuIiKiUsIiIiEoJi4iIqJSwiIiISgmLCEDSP0r6TJN9H5X0/rpriuhPEhYREVEpYRExgEga2uoaYmBKWMQuoxz++WNJ90v6uaSvS9pP0g8lbZB0m6SxDf1nSlom6TlJiyQd3DDvKEn3lstdD4zotq0PSFpaLnunpCOarPFESfdJel7SakmXdpv/rnJ9z5XzzynbR0r6W0mPSVov6Sdl23GSOnr4e3h/+flSSfMkfUvS88A5kqZLuqvcxhOSviRpeMPyh0paKGmdpKck/Zmk/SX9QtK4hn5HS+qUNKyZfY+BLWERu5pTgeOBtwAnAT8E/gyYQPHf8wUAkt4CXAdcVM77AfA9ScPLX5w3A/8E7AN8u1wv5bJHAVcDfwCMA74KzJe0exP1/Rz4MDAGOBH4I0mnlOt9Y1nvF8uajgSWlst9HjgG+LWypj8BXm7y7+RkYF65zWuBl4CPA+OBdwDvAz5W1jAauA24FTgQeDNwu+0ngUXA6Q3r/RAw1/aWJuuIASxhEbuaL9p+yvYa4N+Ae2zfZ3szcBNwVNlvFnCL7YXlL7vPAyMpfhkfCwwD/t72FtvzgMUN25gNfNX2PbZfsn0N8EK5XK9sL7L9gO2Xbd9PEVi/Uc4+E7jN9nXldtfaXippN+D3gAttrym3eaftF5r8O7nL9s3lNjfZXmL7bttbbT9KEXZdNXwAeNL239rebHuD7XvKedcAZwNIGgL8LkWgRiQsYpfzVMPnTT1M71l+PhB4rGuG7ZeB1cDEct4av/Ipmo81fH4jcHE5jPOcpOeAyeVyvZL0dkl3lMM364E/pPgXPuU6ftbDYuMphsF6mteM1d1qeIuk70t6shya+j9N1ADwXeAQSVMpzt7W2/6PHawpBpiERQxUj1P80gdAkih+Ua4BngAmlm1d3tDweTXwV7bHNPyMsn1dE9v9Z2A+MNn23sBXgK7trAZ+pYdlngE2b2fez4FRDfsxhGIIq1H3R0d/GXgImGZ7L4phusYa3tRT4eXZ2Q0UZxcfImcV0SBhEQPVDcCJkt5XXqC9mGIo6U7gLmArcIGkYZJ+G5jesOzXgD8szxIkaY/ywvXoJrY7Glhne7Ok6RRDT12uBd4v6XRJQyWNk3RkedZzNXC5pAMlDZH0jvIayU+BEeX2hwF/AVRdOxkNPA9slPRW4I8a5n0fOEDSRZJ2lzRa0tsb5n8TOAeYScIiGiQsYkCy/TDFv5C/SPEv95OAk2y/aPtF4Lcpfimuo7i+cWPDsu3A7wNfAp4FVpV9m/Ex4DJJG4C/pAitrvX+N/BbFMG1juLi9q+Wsy8BHqC4drIO+Bywm+315Tqvojgr+jnwim9H9eASipDaQBF81zfUsIFiiOkk4ElgJfCehvn/TnFh/V7bjUNzMcgpLz+KiEaS/hX4Z9tXtbqW6D8SFhGxjaS3AQsprrlsaHU90X9kGCoiAJB0DcU9GBclKKK7nFlERESlnFlERESlAfPQsfHjx3vKlCmtLiMiYpeyZMmSZ2x3v3fnVQZMWEyZMoX29vZWlxERsUuR1NRXpDMMFRERlRIWERFRqdawkDRD0sOSVkma08P8c8oHri0tf85tmPcGSf8iaYWk5ZKm1FlrRERsX23XLMoHnl1B8WiBDmCxpPm2l3frer3t83tYxTcpHua2UNKeNP9s/222bNlCR0cHmzdvfq2L7nJGjBjBpEmTGDYs76mJiL5X5wXu6cAq248ASJpL8ZKW7mHxKpIOAYbaXghge+OOFNDR0cHo0aOZMmUKr3zA6MBim7Vr19LR0cHUqVNbXU5EDEB1DkNN5JXP2e8o27o7VcVrMudJmly2vQV4TtKN5Ssq/295pvIKkmZLapfU3tnZ+aoVb968mXHjxg3ooACQxLhx4wbFGVREtEarL3B/D5hi+wiK59FcU7YPBd5N8fTMt1E8f/+c7gvbvtJ2m+22CRN6/prwQA+KLoNlPyOiNeochlpD8bKZLpPKtm1sr22YvAr4m/JzB7C0YQjrZopXWn69tmq7sc36TVvYvOU1Xyppmec3beHyf3m41WVExE62/94jOfPtb6ju+DrUGRaLgWnlKxrXAGfwyhfBIOkA20+UkzOBFQ3LjpE0wXYn8F5gp91xZ5unnn+Bpze8/mGd59ev54c3f5tZHzm3unOD8z58Gn/9xavYa++9m15mw+atfPGO1dUdI2JAOXLymF03LGxvlXQ+sAAYAlxte5mky4B22/Mp3lQ2k+KtZesoh5psvyTpEuD28tWXSyhe4lK7xqDYZ4/hTBwz8nUN8Ty69Tm+e90/8ld/fskr2rdu3crQodv/6/+3f134mre1YsNI/uuvT3zNy0VEVKn1cR+2fwD8oFvbXzZ8/gTwie0suxA4os76etKXQQEwZ84cfvazn3HkkUcybNgwRowYwdixY3nooYf46U9/yimnnMLq1avZvHkzF154IbNnzwZ++fiSjRs3csIJJ/Cud72LO++8k4kTJ/Ld736XkSNH9sXuRkQ0ZcA8G6rKp763jOWPP99rH9v84sWXGDpkN3YfWn3t/5AD9+KTJx3aa5/PfvazPPjggyxdupRFixZx4okn8uCDD277iuvVV1/NPvvsw6ZNm3jb297Gqaeeyrhx416xjpUrV3Ldddfxta99jdNPP53vfOc7nH322ZX1RUT0lUETFs3Y+nLxbo/hQ+r7ZtH06dNfcS/EF77wBW666SYAVq9ezcqVK18VFlOnTuXII48E4JhjjuHRRx+trb6IiJ4MmrCoOgMAWPl08XKwafuOrq2OPfbYY9vnRYsWcdttt3HXXXcxatQojjvuuB7vldh99923fR4yZAibNm2qrb6IiJ60+j6LfuOFrS+x6cWXGDOybx+XMXr0aDZs6PkNlevXr2fs2LGMGjWKhx56iLvvvrtPtx0R0VcGzZlFlfWbtgCwdx+Hxbhx43jnO9/JYYcdxsiRI9lvv/22zZsxYwZf+cpXOPjggznooIM49thj+3TbERF9ZcC8g7utrc3dX360YsUKDj744KaWX/nUBiTx5n33rKO8neK17G9EBICkJbbbqvplGAp4YctLbNryUp+fVUREDBQJCxqHoDIqFxHRk4QFRViMGj6U4UNf9WDbiIggYVF8CypDUBERvRr04y7Dh+zGtH1HM6zGG/EiInZ1gz4sJDFyeIafIiJ6M+iHofqbPffcdb+6GxEDV8IiIiIqDfphqLrNmTOHyZMnc9555wFw6aWXMnToUO644w6effZZtmzZwmc+8xlOPvnkFlcaEbF9gycsfjgHnnygb9e5/+Fwwmd77TJr1iwuuuiibWFxww03sGDBAi644AL22msvnnnmGY499lhmzpyZ92hHRL81eMKiRY466iiefvppHn/8cTo7Oxk7diz7778/H//4x/nxj3/Mbrvtxpo1a3jqqafYf//9W11uRESPBk9YVJwB1Om0005j3rx5PPnkk8yaNYtrr72Wzs5OlixZwrBhw5gyZUqPjyaPiOgvar3ALWmGpIclrZI0p4f550jqlLS0/Dm32/y9JHVI+lKdddZt1qxZzJ07l3nz5nHaaaexfv169t13X4YNG8Ydd9zBY4891uoSIyJ6VduZhaQhwBXA8UAHsFjSfNvLu3W93vb521nNp4Ef11XjznLooYeyYcMGJk6cyAEHHMBZZ53FSSedxOGHH05bWxtvfetbW11iRESv6hyGmg6ssv0IgKS5wMlA97DokaRjgP2AW4HKx+f2dw888MuL6+PHj+euu+7qsd/GjRt3VkkREU2rcxhqIrC6YbqjbOvuVEn3S5onaTKApN2AvwUuqbG+iIhoUqtvyvseMMX2EcBC4Jqy/WPAD2x39LawpNmS2iW1d3Z21lxqRMTgVecw1BpgcsP0pLJtG9trGyavAv6m/PwO4N2SPgbsCQyXtNH2nG7LXwlcCcWb8noqwvaguH9hoLzxMCL6pzrDYjEwTdJUipA4AzizsYOkA2w/UU7OBFYA2D6roc85QFv3oGjGiBEjWLt2LePGjRvQgWGbtWvXMmLEiFaXEhEDVG1hYXurpPOBBcAQ4GrbyyRdBrTbng9cIGkmsBVYB5zTlzVMmjSJjo4OBsMQ1YgRI5g0aVKry4iIAUoDZfiira3N7e3trS4jImKXImmJ7cpvnLb6AndEROwCEhYREVEpYREREZUSFhERUSlhERERlRIWERFRKWERERGVEhYREVEpYREREZUSFhERUSlhERERlRIWERFRKWERERGVEhYREVEpYREREZUSFhERUSlhERERlRIWERFRKWERERGVag0LSTMkPSxplaQ5Pcw/R1KnpKXlz7ll+5GS7pK0TNL9kmbVWWdERPRuaF0rljQEuAI4HugAFkuab3t5t67X2z6/W9svgA/bXinpQGCJpAW2n6ur3oiI2L46zyymA6tsP2L7RWAucHIzC9r+qe2V5efHgaeBCbVVGhERvaozLCYCqxumO8q27k4th5rmSZrcfaak6cBw4Gc9zJstqV1Se2dnZ1/VHRER3bT6Avf3gCm2jwAWAtc0zpR0APBPwEdtv9x9YdtX2m6z3TZhQk48IiLqUmdYrAEazxQmlW3b2F5r+4Vy8irgmK55kvYCbgH+3PbdNdYZEREV6gyLxcA0SVMlDQfOAOY3dijPHLrMBFaU7cOBm4Bv2p5XY40REdGE2r4NZXurpPOBBcAQ4GrbyyRdBrTbng9cIGkmsBVYB5xTLn468OvAOEldbefYXlpXvRERsX2y3eoa+kRbW5vb29tbXUZExC5F0hLbbVX9Wn2BOyIidgEJi4iIqJSwiIiISgmLiIiolLCIiIhKCYuIiKiUsIiIiEoJi4iIqJSwiIiISgmLiIiolLCIiIhKCYuIiKiUsIiIiEoJi4iIqJSwiIiISgmLiIiolLCIiIhKCYuIiKhUa1hImiHpYUmrJM3pYf45kjolLS1/zm2Y9xFJK8ufj9RZZ0RE9G5oXSuWNAS4Ajge6AAWS5pve3m3rtfbPr/bsvsAnwTaAANLymWfraveiIjYvjrPLKYDq2w/YvtFYC5wcpPL/g9goe11ZUAsBGbUVGdERFRoKiwk3SjpREmvJVwmAqsbpjvKtu5OlXS/pHmSJr/GZSMiYido9pf//wPOBFZK+qykg/po+98Dptg+guLs4ZrXsrCk2ZLaJbV3dnb2UUkREdFdU2Fh+zbbZwFHA48Ct0m6U9JHJQ3bzmJrgMkN05PKtsb1rrX9Qjl5FXBMs8uWy19pu81224QJE5rZlYiI2AFNDytJGgecA5wL3Af8A0V4LNzOIouBaZKmShoOnAHM77bOAxomZwIrys8LgN+UNFbSWOA3y7aIiGiBpr4NJekm4CDgn4CTbD9RzrpeUntPy9jeKul8il/yQ4CrbS+TdBnQbns+cIGkmcBWYB1FGGF7naRPUwQOwGW21+3QHkZExOsm29WdpPfYvmMn1LPD2tra3N7eY25FRMR2SFpiu62qX7PDUIdIGtOw8rGSPrbD1UVExC6l2bD4fdvPdU2U9z78fj0lRUREf9NsWAyRpK6J8u7s4fWUFBER/U2zj/u4leJi9lfL6T8o2yIiYhBoNiz+lCIg/qicXkhxX0RERAwCTYWF7ZeBL5c/ERExyDR7n8U04K+BQ4ARXe2231RTXRER0Y80e4H7GxRnFVuB9wDfBL5VV1EREdG/NBsWI23fTnET32O2LwVOrK+siIjoT5q9wP1C+XjyleUjPNYAe9ZXVkRE9CfNnllcCIwCLqB4MuzZQF51GhExSFSeWZQ34M2yfQmwEfho7VVFRES/UnlmYfsl4F07oZaIiOinmr1mcZ+k+cC3gZ93Ndq+sZaqIiKiX2k2LEYAa4H3NrQZSFhERAwCzd7BnesUERGDWLN3cH+D4kziFWz/Xp9XFBER/U6zw1Dfb/g8Avgg8HjflxMREf1Rs8NQ32mclnQd8JNaKoqIiH6n2ZvyupsG7FvVSdIMSQ9LWiVpTi/9TpVkSW3l9DBJ10h6QNIKSZ/YwTojIqIPNHvNYgOvvGbxJMU7LnpbZghwBXA80AEsljTf9vJu/UZT3CF+T0PzacDutg+XNApYLuk62482U29ERPStZoehRu/AuqcDq2w/AiBpLnAysLxbv08DnwP+uHGTwB6ShgIjgReB53eghoiI6ANNDUNJ+qCkvRumx0g6pWKxicDqhumOsq1xvUcDk23f0m3ZeRQ3/z0B/DfwedvreqhrtqR2Se2dnZ3N7EpEROyAZq9ZfNL2+q4J288Bn3w9Gy6fYns5cHEPs6cDLwEHAlOBiyW96kVLtq+03Wa7bcKECa+nnIiI6EWzX53tKVSqll0DTG6YnlS2dRkNHAYskgSwPzBf0kzgTOBW21uApyX9O9AGPNJkvRER0YeaPbNol3S5pF8pfy4HllQssxiYJmmqpOHAGcD8rpm219seb3uK7SnA3cBM2+0UQ0/vBZC0B3As8NBr2rOIiOgzzYbF/6K4yHw9MBfYDJzX2wK2twLnAwuAFcANtpdJuqw8e+jNFcCekpZRhM43bN/fZK0REdHHZL/qKR67pLa2Nre3t7e6jIiIXYqkJbbbqvo1+22ohZLGNEyPlbTg9RQYERG7jmaHocaX34ACwPazNHEHd0REDAzNhsXLkt7QNSFpCj08hTYiIgamZr86++fATyT9CBDwbmB2bVVFRES/0uzjPm4tH/I3G7gPuBnYVGdhERHRfzT7IMFzKR72NwlYSnHfw1288jWrERExQDV7zeJC4G3AY7bfAxwFPNf7IhERMVA0GxabbW8GkLS77YeAg+orKyIi+pNmL3B3lPdZ3AwslPQs8Fh9ZUVERH/S7AXuD5YfL5V0B7A3cGttVUVERL/S7JnFNrZ/VEchERHRf+3oO7gjImIQSVhERESlhEVERFRKWERERKWERUREVEpYREREpYRFRERUqjUsJM2Q9LCkVZLm9NLvVEkun2zb1XaEpLskLZP0gKQRddYaERHb95pvymuWpCHAFcDxQAewWNJ828u79RtN8aDCexrahgLfAj5k+z8ljQO21FVrRET0rs4zi+nAKtuP2H4RmAuc3EO/TwOfAzY3tP0mcL/t/wSwvdb2SzXWGhERvagzLCYCqxumO8q2bSQdDUy2fUu3Zd8CWNICSfdK+pOeNiBptqR2Se2dnZ19WXtERDRo2QVuSbsBlwMX9zB7KPAu4Kzyzw9Kel/3TravtN1mu23ChAm11hsRMZjVGRZrgMkN05PKti6jgcOARZIepXj73vzyIncH8GPbz9j+BfAD4Ogaa42IiF7UGRaLgWmSpkoaDpwBzO+aaXu97fG2p9ieAtwNzLTdDiwADpc0qrzY/RvA8ldvIiIidobawsL2VuB8il/8K4AbbC+TdJmkmRXLPksxRLWY4p3f9/ZwXSMiInYS2W51DX2ira3N7e3trS4jImKXImmJ7baqfrmDOyIiKiUsIiKiUsIiIiIqJSwiIqJSwiIiIiolLCIiolLCIiIiKiUsIiKiUsIiIiIqJSwiIqJSwiIiIiolLCIiolLCIiIiKiUsIiKiUsIiIiIqJSwiIqJSwiIiIiolLCIiolLCIiIiKtUaFpJmSHpY0ipJc3rpd6okS2rr1v4GSRslXVJnnRER0bvawkLSEOAK4ATgEOB3JR3SQ7/RwIXAPT2s5nLgh3XVGBERzanzzGI6sMr2I7ZfBOYCJ/fQ79PA54DNjY2STgH+C1hWY40REdGEOsNiIrC6YbqjbNtG0tHAZNu3dGvfE/hT4FO9bUDSbEntkto7Ozv7puqIiHiVll3glrQbxTDTxT3MvhT4O9sbe1uH7Sttt9lumzBhQg1VRkQEwNAa170GmNwwPals6zIaOAxYJAlgf2C+pJnA24HfkfQ3wBjgZUmbbX+pxnojImI76gyLxcA0SVMpQuIM4MyumbbXA+O7piUtAi6x3Q68u6H9UmBjgiIionVqG4ayvRU4H1gArABusL1M0mXl2UNEROwiZLvVNfSJtrY2t7e3t7qMiIhdiqQlttuq+uUO7oiIqJSwiIiISgmLiIiolLCIiIhKCYuIiKiUsIiIiEoJi4iIqJSwiIiISgmLiIiolLCIiIhKCYuIiKiUsIiIiEoJi4iIqJSwiIiISgmLiIiolLCIiIhKCYuIiKiUsIiIiEq1hoWkGZIelrRK0pxe+p0qyZLayunjJS2R9ED553vrrDMiIno3tK4VSxoCXAEcD3QAiyXNt728W7/RwIXAPQ3NzwAn2X5c0mHAAmBiXbVGRETv6jyzmA6ssv2I7ReBucDJPfT7NPA5YHNXg+37bD9eTi4DRkravcZaIyKiF3WGxURgdcN0B93ODiQdDUy2fUsv6zkVuNf2C91nSJotqV1Se2dnZ1/UHBERPWjZBW5JuwGXAxf30udQirOOP+hpvu0rbbfZbpswYUI9hUZERK1hsQaY3DA9qWzrMho4DFgk6VHgWGB+w0XuScBNwIdt/6zGOiMiokKdYbEYmCZpqqThwBnA/K6ZttfbHm97iu0pwN3ATNvtksYAtwBzbP97jTVGREQTagsL21uB8ym+ybQCuMH2MkmXSZpZsfj5wJuBv5S0tPzZt65aIyKid7Ld6hr6RFtbm9vb21tdRkTELkXSEtttVf1yB3dERFRKWERERKXa7uDepfxwDjz5QKuriIjYMfsfDid8ttZN5MwiIiIq5cwCak/kiIhdXc4sIiKiUsIiIiIqJSwiIqJSwiIiIiolLCIiolLCIiIiKiUsIiKiUsIiIiIqDZinzkrqBB57HasYDzzTR+XsKgbjPsPg3O/BuM8wOPf7te7zG21Xvmp0wITF6yWpvZnH9A4kg3GfYXDu92DcZxic+13XPmcYKiIiKiUsIiKiUsLil65sdQEtMBj3GQbnfg/GfYbBud+17HOuWURERKWcWURERKWERUREVBr0YSFphqSHJa2SNKfV9dRF0mRJd0haLmmZpAvL9n0kLZS0svxzbKtr7WuShki6T9L3y+mpku4pj/n1koa3usa+JmmMpHmSHpK0QtI7BvqxlvTx8r/tByVdJ2nEQDzWkq6W9LSkBxvaejy2Knyh3P/7JR29o9sd1GEhaQhwBXACcAjwu5IOaW1VtdkKXGz7EOBY4LxyX+cAt9ueBtxeTg80FwIrGqY/B/yd7TcDzwL/syVV1esfgFttvxX4VYr9H7DHWtJE4AKgzfZhwBDgDAbmsf5HYEa3tu0d2xOAaeXPbODLO7rRQR0WwHRgle1HbL8IzAVObnFNtbD9hO17y88bKH55TKTY32vKbtcAp7SmwnpImgScCFxVTgt4LzCv7DIQ93lv4NeBrwPYftH2cwzwY03xmuiRkoYCo4AnGIDH2vaPgXXdmrd3bE8GvunC3cAYSQfsyHYHe1hMBFY3THeUbQOapCnAUcA9wH62nyhnPQns16Ky6vL3wJ8AL5fT44DnbG8tpwfiMZ8KdALfKIffrpK0BwP4WNteA3we+G+KkFgPLGHgH+su2zu2ffY7brCHxaAjaU/gO8BFtp9vnOfie9QD5rvUkj4APG17Satr2cmGAkcDX7Z9FPBzug05DcBjPZbiX9FTgQOBPXj1UM2gUNexHexhsQaY3DA9qWwbkCQNowiKa23fWDY/1XVaWv75dKvqq8E7gZmSHqUYYnwvxVj+mHKoAgbmMe8AOmzfU07PowiPgXys3w/8l+1O21uAGymO/0A/1l22d2z77HfcYA+LxcC08hsTwykuiM1vcU21KMfqvw6ssH15w6z5wEfKzx8Bvruza6uL7U/YnmR7CsWx/VfbZwF3AL9TdhtQ+wxg+0lgtaSDyqb3AcsZwMeaYvjpWEmjyv/Wu/Z5QB/rBts7tvOBD5ffijoWWN8wXPWaDPo7uCX9FsW49hDgatt/1eKSaiHpXcC/AQ/wy/H7P6O4bnED8AaKR7yfbrv7xbNdnqTjgEtsf0DSmyjONPYB7gPOtv1CK+vra5KOpLioPxx4BPgoxT8OB+yxlvQpYBbFN//uA86lGJ8fUMda0nXAcRSPIn8K+CRwMz0c2zI4v0QxJPcL4KO223dou4M9LCIiotpgH4aKiIgmJCwiIqJSwiIiIiolLCIiolLCIiIiKiUsIvoBScd1PRU3oj9KWERERKWERcRrIOlsSf8haamkr5bvytgo6e/KdyncLmlC2fdISXeX7xG4qeEdA2+WdJuk/5R0r6RfKVe/Z8M7KK4tb6iK6BcSFhFNknQwxR3C77R9JPAScBbFQ+vabR8K/IjijlqAbwJ/avsIijvnu9qvBa6w/avAr1E8JRWKJwFfRPFulTdRPNsool8YWt0lIkrvA44BFpf/6B9J8cC2l4Hryz7fAm4s3ykxxvaPyvZrgG9LGg1MtH0TgO3NAOX6/sN2Rzm9FJgC/KT+3YqolrCIaJ6Aa2x/4hWN0v/u1m9Hn6HT+Myil8j/n9GPZBgqonm3A78jaV/Y9t7jN1L8f9T1ZNMzgZ/YXg88K+ndZfuHgB+VbynskHRKuY7dJY3aqXsRsQPyL5eIJtleLukvgH+RtBuwBTiP4uVC08t5T1Nc14DiUdFfKcOg68mvUATHVyVdVq7jtJ24GxE7JE+djXidJG20vWer64ioU4ahIiKiUs4sIiKiUs4sIiKiUsIiIiIqJSwiIqJSwiIiIiolLCIiotL/B28Y9F2BDT84AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAAGSlJREFUeJzt3X20HXV97/H3hySQB4KEEB5MwKSVKg9qKEcaHxdL1CWo4CoK9EJbuS1oy1qA17YXe/tgu2yrt65ra7EiCq22XCryUGmLSlFAewVqEqkGQQkVTMJDDhHyAEQS+N4/9jA9HE+Sk4d9dtj7/Vprr7P3zG/PfIcJ53PmNzO/SVUhSRLAHr0uQJK0+zAUJEktQ0GS1DIUJEktQ0GS1DIUJEktQ0EapyR/m+RD42x7X5I37uxypIlmKEiSWoaCJKllKKivNN02v53kO0keT3JpkgOTfCnJ+iQ3Jpk1ov1JSe5M8liSm5McPmLe0UmWNt/7PDB11LreluSO5rvfTPLyHaz57CTLk/w4yXVJXthMT5KPJVmdZF2S7yY5qpl3YpLvNbWtSvJbO/QfTBrFUFA/OgV4E/BzwNuBLwG/C8yh82/+PIAkPwdcAVzQzLse+KckeybZE/hH4O+A/YAvNMul+e7RwGXAe4DZwKeA65LstT2FJnkD8GfAqcDBwP3APzSz3wy8vtmOFzRt1jTzLgXeU1UzgaOAr23PeqUtMRTUj/6qqh6uqlXAN4Dbq+rbVbURuBY4uml3GvAvVfWvVbUJ+CgwDXg1sAiYAvxFVW2qqquAb41YxznAp6rq9qp6uqo+C/yk+d72OAO4rKqWVtVPgA8Ar0oyH9gEzAReCqSq7qqqB5vvbQKOSLJPVT1aVUu3c73SmAwF9aOHR7x/cozPezfvX0jnL3MAquoZYAUwt5m3qp47YuT9I96/CHh/03X0WJLHgEOa722P0TVsoHM0MLeqvgZcBHwCWJ3kkiT7NE1PAU4E7k9yS5JXbed6pTEZChpkD9D55Q50+vDp/GJfBTwIzG2mPevQEe9XAH9SVfuOeE2vqit2soYZdLqjVgFU1cer6hjgCDrdSL/dTP9WVZ0MHECnm+vK7VyvNCZDQYPsSuCtSY5PMgV4P50uoG8CtwKbgfOSTEnyi8CxI777aeC9SX6hOSE8I8lbk8zczhquAM5KsrA5H/GndLq77kvyymb5U4DHgY3AM805jzOSvKDp9loHPLMT/x2klqGggVVV3wfOBP4KeITOSem3V9VTVfUU8IvAu4Ef0zn/cM2I7y4GzqbTvfMosLxpu7013Aj8PnA1naOTnwVOb2bvQyd8HqXTxbQG+PNm3i8D9yVZB7yXzrkJaafFh+xIkp7lkYIkqWUoSJJahoIkqdXVUEhyfpJlzTACF4wx/7gka5uhAu5I8gfdrEeStHWTu7XgZoyWs+lcxvcU8OUk/1xVy0c1/UZVvW28y91///1r/vz5u65QSRoAS5YseaSq5myrXddCATiczvXWTwAkuYXOJX7/e2cWOn/+fBYvXrwLypOkwZHk/m236m730TLgdUlmJ5lO55b8Q8Zo96ok/9GMYnnkWAtKck6SxUkWDw8Pd7FkSRpsXTtSqKq7knwEuIHO3Zh3AE+ParYUeFFVbUhyIp3b9Q8bY1mXAJcADA0NeWOFJHVJV080V9WlVXVMVb2ezl2ZPxg1f10zABhVdT0wJcn+3axJkrRl3TynQJIDqmp1kkPpnE9YNGr+QcDDVVVJjqUTUmvGWNRWbdq0iZUrV7Jx48ZdUvfubOrUqcybN48pU6b0uhRJfairoQBcnWQ2nbHfz62qx5K8F6CqLgbeCfxGks10hjQ+vXZg3I2VK1cyc+ZM5s+fz3MHtewvVcWaNWtYuXIlCxYs6HU5kvpQV0Ohql43xrSLR7y/iM6AYjtl48aNfR8IAEmYPXs2nmyX1C19c0dzvwfCswZlOyX1Rre7j3Yfa1fCpid7XcWusWE1/I3PaZcGzkEvgxM+3NVV9M2RQi89tnYdf33Z5dv9vRNP/3UeW7uuCxVJ0o4ZnCOFF8zr2qIf23Aff/25q/jN3/ngc6Zv3ryZyZO3/J/4+htv2bEVDm+Gs/5lx74rSVsxOKHQRRdeeCH33nsvCxcuZMqUKUydOpVZs2Zx991384Mf/IB3vOMdrFixgo0bN3L++edzzjnnAP81ZMeGDRs44YQTeO1rX8s3v/lN5s6dyxe/+EWmTZvW4y2TNGj6LhT+6J/u5HsP7NoumSNeuA9/+PYxR+AA4MMf/jDLli3jjjvu4Oabb+atb30ry5Ytay8bveyyy9hvv/148skneeUrX8kpp5zC7Nmzn7OMe+65hyuuuIJPf/rTnHrqqVx99dWceeaZu3Q7JGlb+i4UdgfHHnvsc+4j+PjHP861114LwIoVK7jnnnt+KhQWLFjAwoULATjmmGO47777JqxeSXpW34XC1v6inygzZsxo3998883ceOON3HrrrUyfPp3jjjtuzDuv99prr/b9pEmTePLJPrlSStLzilcf7QIzZ85k/fr1Y85bu3Yts2bNYvr06dx9993cdtttE1ydJI1f3x0p9MLs2bN5zWtew1FHHcW0adM48MAD23lvectbuPjiizn88MN5yUtewqJFi7ayJEnqrezAUEM9NTQ0VKMfsnPXXXdx+OGH96iiiTdo2ytp5yVZUlVD22pn95EkqWUoSJJahoIkqWUoSJJahoIkqWUoSJJahkIP7L333r0uQZLGZChIklre0bwLXHjhhRxyyCGce+65AHzwgx9k8uTJ3HTTTTz66KNs2rSJD33oQ5x88sk9rlSStq7/QuFLF8JD3921y9zGI/BOO+00LrjggjYUrrzySr7yla9w3nnnsc8++/DII4+waNEiTjrpJJ+xLGm31n+h0ANHH300q1ev5oEHHmB4eJhZs2Zx0EEH8b73vY+vf/3r7LHHHqxatYqHH36Ygw46qNflStIW9V8odPmh1lvyrne9i6uuuoqHHnqI0047jcsvv5zh4WGWLFnClClTmD9//phDZkvS7qT/QqFHTjvtNM4++2weeeQRbrnlFq688koOOOAApkyZwk033cT999/f6xIlaZsMhV3kyCOPZP369cydO5eDDz6YM844g7e//e287GUvY2hoiJe+9KW9LlGStslQ2IW++93/OsG9//77c+utt47ZbsOGDRNVkiRtF+9TkCS1DAVJUqtvQuH59gS5HTUo2ympN/oiFKZOncqaNWv6/hdmVbFmzRqmTp3a61Ik9amunmhOcj5wNhDg01X1F6PmB/hL4ETgCeDdVbV0e9czb948Vq5cyfDw8C6oevc2depU5s2b1+syJPWproVCkqPoBMKxwFPAl5P8c1UtH9HsBOCw5vULwCebn9tlypQpLFiwYOeLlqQB183uo8OB26vqiaraDNwC/OKoNicDn6uO24B9kxzcxZokSVvRzVBYBrwuyewk0+l0ER0yqs1cYMWIzyubac+R5Jwki5MsHoQuIknqla6FQlXdBXwEuAH4MnAH8PQOLuuSqhqqqqE5c+bswiolSSN19eqjqrq0qo6pqtcDjwI/GNVkFc89epjXTJMk9UBXQyHJAc3PQ+mcT/i/o5pcB/xKOhYBa6vqwW7WJEnasm6PfXR1ktnAJuDcqnosyXsBqupi4Ho65xqW07kk9awu1yNJ2oquhkJVvW6MaRePeF/Aud2sQZI0fn1xR7MkadcwFCRJLUNBktQyFCRJLUNBktQyFCRJLUNBktQyFCRJLUNBktQyFCRJLUNBktQyFCRJLUNBktQyFCRJLUNBktQyFCRJLUNBktQyFCRJLUNBktQyFCRJLUNBktQyFCRJLUNBktQyFCRJLUNBktQyFCRJLUNBktQyFCRJLUNBktQyFCRJra6GQpL3JbkzybIkVySZOmr+u5MMJ7mjef16N+uRJG1d10IhyVzgPGCoqo4CJgGnj9H081W1sHl9plv1SJK2rdvdR5OBaUkmA9OBB7q8PknSTuhaKFTVKuCjwI+AB4G1VXXDGE1PSfKdJFclOWSsZSU5J8niJIuHh4e7VbIkDbxudh/NAk4GFgAvBGYkOXNUs38C5lfVy4F/BT471rKq6pKqGqqqoTlz5nSrZEkaeN3sPnoj8MOqGq6qTcA1wKtHNqiqNVX1k+bjZ4BjuliPJGkbuhkKPwIWJZmeJMDxwF0jGyQ5eMTHk0bPlyRNrMndWnBV3Z7kKmApsBn4NnBJkj8GFlfVdcB5SU5q5v8YeHe36pEkbVuqqtc1bJehoaFavHhxr8uQpOeVJEuqamhb7byjWZLUMhQkSS1DQZLUMhQkSS1DQZLUMhQkSS1DQZLUMhQkSS1DQZLUMhQkSS1DQZLUMhQkSS1DQZLUMhQkSS1DQZLUMhQkSS1DQZLUGlcoJDk/yT7puDTJ0iRv7nZxkqSJNd4jhf9eVeuANwOzgF8GPty1qiRJPTHeUEjz80Tg76rqzhHTJEl9YryhsCTJDXRC4StJZgLPdK8sSVIvTB5nu18DFgL/WVVPJNkPOKt7ZUmSemG8RwqvAr5fVY8lORP4PWBt98qSJPXCeEPhk8ATSV4BvB+4F/hc16qSJPXEeENhc1UVcDJwUVV9ApjZvbIkSb0w3nMK65N8gM6lqK9LsgcwpXtlSZJ6YbxHCqcBP6Fzv8JDwDzgz7tWlSSpJ8YVCk0QXA68IMnbgI1V5TkFSeoz4x3m4lTg34F3AacCtyd5ZzcLkyRNvPGeU/hfwCurajVAkjnAjcBV3SpMkjTxxntOYY9nA6GxZjzfTfK+JHcmWZbkiiRTR83fK8nnkyxPcnuS+eOuXJK0y403FL6c5CtJ3p3k3cC/ANdv7QtJ5gLnAUNVdRQwCTh9VLNfAx6tqhcDHwM+sj3FS5J2rXF1H1XVbyc5BXhNM+mSqrp2nMuflmQTMB14YNT8k4EPNu+vAi5KkuaeCEnSBBvvOQWq6mrg6u1ovyrJR4EfAU8CN1TVDaOazQVWNO03J1kLzAYeGdkoyTnAOQCHHnroeEuQJG2nrXYfJVmfZN0Yr/VJ1m3ju7PoHAksAF4IzGjGTdpuVXVJVQ1V1dCcOXN2ZBGSpHHY6pFCVe3MUBZvBH5YVcMASa4BXg38/Yg2q4BDgJVJJgMvoHMSW5LUA918RvOPgEVJpicJcDxw16g21wG/2rx/J/A1zydIUu90LRSq6nY6J4+XAt9t1nVJkj9OclLT7FJgdpLlwP8ALuxWPZKkbcvz7Q/zoaGhWrx4ca/LkKTnlSRLqmpoW+262X0kSXqeMRQkSS1DQZLUMhQkSS1DQZLUMhQkSS1DQZLUMhQkSS1DQZLUMhQkSS1DQZLUMhQkSS1DQZLUMhQkSS1DQZLUMhQkSS1DQZLUMhQkSS1DQZLUMhQkSS1DQZLUMhQkSS1DQZLUMhQkSS1DQZLUMhQkSS1DQZLUMhQkSS1DQZLUMhQkSa2uhUKSlyS5Y8RrXZILRrU5LsnaEW3+oFv1SJK2bXK3FlxV3wcWAiSZBKwCrh2j6Teq6m3dqkOSNH4T1X10PHBvVd0/QeuTJO2AiQqF04ErtjDvVUn+I8mXkhw5VoMk5yRZnGTx8PBw96qUpAHX9VBIsidwEvCFMWYvBV5UVa8A/gr4x7GWUVWXVNVQVQ3NmTOne8VK0oCbiCOFE4ClVfXw6BlVta6qNjTvrwemJNl/AmqSJI1hIkLhl9hC11GSg5KkeX9sU8+aCahJkjSGrl19BJBkBvAm4D0jpr0XoKouBt4J/EaSzcCTwOlVVd2sSZK0ZV0Nhap6HJg9atrFI95fBFzUzRokSePnHc2SpJahIElqGQqSpJahIElqGQqSpJahIElqGQqSpJahIElqGQqSpJahIElqGQqSpJahIElqGQqSpJahIElqGQqSpJahIElqGQqSpJahIElqGQqSpJahIElqGQqSpJahIElqGQqSpJahIElqDWQorNnwE37j75fwN//vhzz6+FO9LkeSdhuTe11AL/zb8kf40rKH+NKyh/iz6+/mDS89gAP22avXZUnSVr32xfvz5iMP6uo6BjIU7nl4A5P2CNf+5qu5ZukqvnLnQ2zc9HSvy5KkrZo9Yy9DoRuWr97Ai2ZP5+Xz9uXl8/blgycd2euSJGm3MJDnFO5ZvZ4Xz9m712VI0m5n4ELhqc3PcP+aJzjsQENBkkbrWigkeUmSO0a81iW5YFSbJPl4kuVJvpPk57tVz7PuX/M4m58pDjtgZrdXJUnPO107p1BV3wcWAiSZBKwCrh3V7ATgsOb1C8Anm59dc8/qDQC8+ACPFCRptInqPjoeuLeq7h81/WTgc9VxG7BvkoO7Wcjy1RtI4Gc9pyBJP2WiQuF04Ioxps8FVoz4vLKZ9hxJzkmyOMni4eHhnSrkntUbmLvvNKbtOWmnliNJ/ajroZBkT+Ak4As7uoyquqSqhqpqaM6cOTtVz/LVGzjMriNJGtNEHCmcACytqofHmLcKOGTE53nNtK54+pni3uENHHagJ5klaSwTEQq/xNhdRwDXAb/SXIW0CFhbVQ92q5AVP36CpzY/4z0KkrQFXb2jOckM4E3Ae0ZMey9AVV0MXA+cCCwHngDO6mY9y5+98sh7FCRpTF0Nhap6HJg9atrFI94XcG43axjJy1ElaesG6o7m5as3cOA+e7HP1Cm9LkWSdksDFgrrPUqQpK0YmFCoquZyVK88kqQtGZhQeHDtRh5/6mmPFCRpKwYmFDzJLEnbNjChMGPPSbzpiAO9m1mStmJgnrw2NH8/hubv1+syJGm3NjBHCpKkbTMUJEktQ0GS1DIUJEktQ0GS1DIUJEktQ0GS1DIUJEmtdB5p8PyRZBi4fwe/vj/wyC4s5/liELd7ELcZBnO7B3GbYfu3+0VVtc2H3D/vQmFnJFlcVUO9rmOiDeJ2D+I2w2Bu9yBuM3Rvu+0+kiS1DAVJUmvQQuGSXhfQI4O43YO4zTCY2z2I2wxd2u6BOqcgSdq6QTtSkCRthaEgSWoNTCgkeUuS7ydZnuTCXtfTDUkOSXJTku8luTPJ+c30/ZL8a5J7mp+zel1rNySZlOTbSf65+bwgye3NPv98kj17XeOulGTfJFcluTvJXUleNQj7Osn7mn/fy5JckWRqP+7rJJclWZ1k2YhpY+7fdHy82f7vJPn5HV3vQIRCkknAJ4ATgCOAX0pyRG+r6orNwPur6ghgEXBus50XAl+tqsOArzaf+9H5wF0jPn8E+FhVvRh4FPi1nlTVPX8JfLmqXgq8gs629/W+TjIXOA8YqqqjgEnA6fTnvv5b4C2jpm1p/54AHNa8zgE+uaMrHYhQAI4FllfVf1bVU8A/ACf3uKZdrqoerKqlzfv1dH5JzKWzrZ9tmn0WeEdvKuyeJPOAtwKfaT4HeANwVdOkr7Y7yQuA1wOXAlTVU1X1GAOwr+k8RnhaksnAdOBB+nBfV9XXgR+Pmryl/Xsy8LnquA3YN8nBO7LeQQmFucCKEZ9XNtP6VpL5wNHA7cCBVfVgM+sh4MAeldVNfwH8DvBM83k28FhVbW4+99s+XwAMA3/TdJl9JskM+nxfV9Uq4KPAj+iEwVpgCf29r0fa0v7dZb/jBiUUBkqSvYGrgQuqat3IedW5BrmvrkNO8jZgdVUt6XUtE2gy8PPAJ6vqaOBxRnUV9em+nkXnr+IFwAuBGfx0F8tA6Nb+HZRQWAUcMuLzvGZa30kyhU4gXF5V1zSTH372ULL5ubpX9XXJa4CTktxHp2vwDXT62/dtuhig//b5SmBlVd3efL6KTkj0+75+I/DDqhquqk3ANXT2fz/v65G2tH932e+4QQmFbwGHNVco7EnnxNR1Pa5pl2v60S8F7qqq/zNi1nXArzbvfxX44kTX1k1V9YGqmldV8+ns269V1RnATcA7m2Z9td1V9RCwIslLmknHA9+jz/c1nW6jRUmmN//en93uvt3Xo2xp/14H/EpzFdIiYO2IbqbtMjB3NCc5kU6/8yTgsqr6kx6XtMsleS3wDeC7/Fff+u/SOa9wJXAonWHHT62q0Sew+kKS44Dfqqq3JfkZOkcO+wHfBs6sqp/0sr5dKclCOifW9wT+EziLzh96fb2vk/wRcBqdq+2+Dfw6nf7zvtrXSa4AjqMzRPbDwB8C/8gY+7cJyIvodKU9AZxVVYt3aL2DEgqSpG0blO4jSdI4GAqSpJahIElqGQqSpJahIElqGQrSBEpy3LOjuEq7I0NBktQyFKQxJDkzyb8nuSPJp5pnNWxI8rFmLP+vJpnTtF2Y5LZmHPtrR4xx/+IkNyb5jyRLk/xss/i9RzwH4fLmxiNpt2AoSKMkOZzOHbOvqaqFwNPAGXQGX1tcVUcCt9C5wxTgc8D/rKqX07mb/NnplwOfqKpXAK+mM6ondEavvYDOsz1+hs7YPdJuYfK2m0gD53jgGOBbzR/x0+gMPPYM8Pmmzd8D1zTPNdi3qm5ppn8W+EKSmcDcqroWoKo2AjTL+/eqWtl8vgOYD/xb9zdL2jZDQfppAT5bVR94zsTk90e129ExYkaOyfM0/n+o3YjdR9JP+yrwziQHQPtc3BfR+f/l2ZE4/xvwb1W1Fng0yeua6b8M3NI8+W5lknc0y9gryfQJ3QppB/gXijRKVX0vye8BNyTZA9gEnEvnQTbHNvNW0znvAJ0hjC9ufuk/O1opdALiU0n+uFnGuyZwM6Qd4iip0jgl2VBVe/e6Dqmb7D6SJLU8UpAktTxSkCS1DAVJUstQkCS1DAVJUstQkCS1/j8dSW66b4djQAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# summarize history for accuracy\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 63.0000%\n",
      "Fscore: [0.77300613 0.        ]\n",
      "Predicted    0  All\n",
      "Actual             \n",
      "0           63   63\n",
      "1           37   37\n",
      "All        100  100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "import pandas as pd\n",
    "\n",
    "model.load_weights('/home/aashinshazar/' + bestModelSavedName)\n",
    "\n",
    "# get index of predicted part for each image in the test image set\n",
    "part_predictions = [np.argmax(model.predict(np.expand_dims(tensor, axis=0))) for tensor in test_tensors]\n",
    "\n",
    "# report test accuracy\n",
    "test_accuracy = 100*np.sum(np.array(part_predictions)==np.argmax(test_targets, axis=1))/len(part_predictions)\n",
    "print('Test accuracy: %.4f%%' % test_accuracy)\n",
    "\n",
    "print (\"Fscore:\", f1_score(np.argmax(test_targets, axis=1), part_predictions, average=None) )\n",
    "\n",
    "y_actu = pd.Series(np.argmax(test_targets, axis=1), name='Actual')\n",
    "y_pred = pd.Series(part_predictions, name='Predicted')\n",
    "\n",
    "df_confusion = pd.crosstab(y_actu, y_pred, rownames=['Actual'], colnames=['Predicted'], margins=True)\n",
    "print (df_confusion)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
